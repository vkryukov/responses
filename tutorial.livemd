# Tutorial

```elixir
Mix.install([
  {:responses, path: "~/src/responses"},
  {:kino, "~> 0.11.0"}
])
```

## Introduction

Responses is a [Small Development Kit](https://dashbit.co/blog/sdks-with-req-stripe) for the OpenAI and xAI Responses-compatible APIs. It can get you started in no time, and automatically supports conversation state, structured responses, streaming, function calls, and cost calculations across providers.

If you want an industrial-grade library with deeper orchestration features you should consider one of [LangChain](https://hexdocs.pm/langchain/readme.html), [OpenaiEx](https://hexdocs.pm/openai_ex/OpenaiEx.html), or [Instructor](https://hexdocs.pm/instructor/Instructor.html). If, however, you want to start as quickly as possible without learning a new framework first and still retain the option to pick between OpenAI and xAI, you've come to the right place!

<!-- livebook:{"branch_parent_index":0} -->

## Basic usage

Set the API keys for the providers you plan to call (`OPENAI_API_KEY` for OpenAI, `XAI_API_KEY` for xAI) and add `responses` to your `mix.exs`

<!-- livebook:{"force_markdown":true} -->

```elixir
def deps do
  [
    # ...
    {:responses, "~> 0.0.1"}
  ]
end
```

is enough to get you started:

```elixir
alias Responses

# Explicit input and model are required
{:ok, response} = Responses.create(input: "Write me a haiku about Elixir", model: "gpt-4.1-mini")
# -> {:ok, %Responses.Response{text: ..., ...}}

IO.puts(response.text)
```

In general, `create/1` takes a keyword list or map with anything that [Create a model response](https://platform.openai.com/docs/api-reference/responses/create) supports. Always pass `input:` and `model:` explicitly. `create!/1` is a version of `create/1` that raises on errors:

```elixir
response =
  Responses.create!(
    input: [
      %{role: :developer, content: "Talk like a pirate."},
      %{role: :user, content: "Write me a haiku about Elixir"}
    ],
    model: "gpt-5-mini",
    reasoning: %{effort: :low}
  )

IO.puts("#{response.text}\n\nCost: $#{response.cost.total_cost}")
```

### Choosing providers and models

Models can be specified in two ways:

- `"provider:model"` resolves to the named provider directly.
- A bare model name (e.g. `"gpt-5"`, `"grok-3"`) is inferred when the library recognises the prefix (`gpt-*`, `o1*`, `o3*`, `o4-mini*` for OpenAI; `grok-*` for xAI).

```elixir
Responses.create!(input: "Summarise the latest xAI blog post", model: "xai:grok-3")
Responses.create!(input: "Explain sparse autoencoders", model: "gpt-5")
```

When the provider does not support a specific option (for example, xAI currently ignores `instructions`), `Responses` emits a warning but forwards the request unchanged. Silence these notices with `provider_warnings: :ignore` or via `config :responses, :provider_warning_mode, :ignore`.

To inspect upstream `/models` metadata, call `Responses.list_models/1` (or `/2` for filtered results) with a provider identifier:

```elixir
Responses.list_models(:openai)
Responses.list_models(:xai, "grok-4")
```

There is also `create/2` and `create!/2` that take `%Responses.Response{}` as a first argument and keyword list as a second to automatically handle the [conversation state](https://platform.openai.com/docs/guides/conversation-state?api-mode=responses). You can also build prompts with `Responses.Prompt` helpers:

```elixir
alias Responses.Prompt

%{}
|> Prompt.add_developer("Talk like a pirate.")
|> Prompt.add_user("Write me a haiku about Elixir")
|> Map.put(:model, "gpt-4.1-mini")
|> Responses.create!()
|> Responses.create!(input: "Which programming language is this haiku about?")
|> Map.get(:text)
|> IO.puts()
```

Notice that the follow-up response also talks like a pirate because the provider preserves conversation state. In terms of generation settings, only the model, reasoning effort, and text verbosity are preserved across follow-ups; schema is not preserved and must be specified again if needed.

<!-- livebook:{"branch_parent_index":0} -->

## Structured Output

One of the coolest features of the latest generation of OpenAI models is their ability to output [Structured Output](https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses) that **precisely matches** the supplied JSON schema. `create/1` accepts a `schema:` option that allows easy creation of such schema in the required format.

The return result will automatically parse the response into `%Response{parsed: object}`.

```elixir
alias Responses

Responses.create!(
  input: "List facts about first 3 U.S. Presidents",
  schema: %{
    presidents:
      {:array,
       %{
         name: :string,
         birth_year: :integer,
         little_known_facts: {:array, {:string, max_items: 2}}
       }}
  },
  model: "gpt-4.1-mini"
)
|> Map.get(:parsed)
|> Map.get("presidents")
```

The above example should give you enough to get started, and you can read the documentation about [supported schemas](https://platform.openai.com/docs/guides/structured-outputs#supported-schemas).

<!-- livebook:{"branch_parent_index":0} -->

## Streaming

Streaming model responses is a great way to increase interactivity of your application. There are two ways to support streaming with `Responses`:

1. By adding a `callback: callback_fn` option to `create/1`. Here `callback_fn/1` takes a map `%{event: type, data: data}` and should return either `:ok` to continue streaming or `{:error, reason}` to stop.

   The call will be blocked until the streaming ends but will otherwise return the same
   `%Response{}` structure.

2. By calling `stream/1`, which returns an Elixir Stream of `%{event: type, data: data}` objects.

For the supported event types and data format, refer to the [Streaming Responses API](https://platform.openai.com/docs/api-reference/responses-streaming/response) docs.

Here is an example of how this works. We use the `Responses.Stream.text_deltas` helper to transform the stream of events into a stream of text chunks, and use `Kino.Frame` to demonstrate interactive output updates in a Livebook.

```elixir
alias Responses

frame = Kino.Frame.new()
Kino.render(frame)

Responses.stream(
  input: """
  Write a short fairy tale about an Elixir developer who tried to use Java,
  and about the horrors that have ensued.
  """,
  temperature: 0.7,
  model: "gpt-4.1-mini"
)
|> Responses.Stream.text_deltas()
|> Stream.each(fn delta ->
  Kino.Frame.append(frame, Kino.Markdown.new(delta, chunk: true))
end)
|> Stream.run()

:done
```

There is also a `Responses.Stream.json_events/1` helper, which uses the `Jaxon` library to stream JSON events:

```elixir
Responses.stream(
  input: "Tell me about first 2 U.S. Presidents",
  schema: %{presidents: {:array, %{name: :string, birth_year: :integer}}},
  model: "gpt-4.1-mini"
)
|> Responses.Stream.json_events()
|> Stream.each(&IO.inspect/1)
|> Stream.run()
```

<!-- livebook:{"branch_parent_index":0} -->

## Tools

Using OpenAI built-in tools, for example [Web Search](https://platform.openai.com/docs/guides/tools-web-search?api-mode=responses), is simple: just add a `tools:` parameter to `create/1`:

```elixir
alias Responses

Responses.create!(
  input: "Summarize in 3 paragraphs a positive news story from today",
  tools: [%{type: "web_search_preview"}]
)
|> Map.get(:text)
|> IO.puts
```

For [Function calling](https://platform.openai.com/docs/guides/function-calling?api-mode=responses), you can provide a function description. The `Responses.Schema.build_function/3` helper makes this easier:

```elixir
# Using the build_function helper
weather_tool = Responses.Schema.build_function(
  "get_weather",
  "Get current temperature for a given location",
  %{location: {:string, description: "City and country e.g. Bogotá, Colombia"}}
)

response = Responses.create!(
  input: "What is the weather like in Paris today?",
  tools: [weather_tool],
  model: "gpt-4.1-mini"
)

response.function_calls
```

The resulting `%Response{}` has a `:function_calls` field populated with the function calls requested by the model. Our app can now call each function, and we can provide results back to the model.

Manually building a function output item:

```elixir
response
|> Responses.create!(
  input: [%{
    type: "function_call_output",
    call_id: response.function_calls |> List.first() |> Map.get(:call_id),
    output: "15C"
  }],
  model: "gpt-4.1-mini"
)
|> Map.get(:text)
```

Alternatively, use the helper to execute and append function outputs:

```elixir
alias Responses.Prompt

functions = %{
  "get_weather" => fn %{"location" => location} -> "15C in #{location}" end
}

opts = Prompt.add_function_outputs(%{input: []}, response.function_calls, functions)

response
|> Responses.create!(opts)
|> Map.get(:text)
```

### Automating Function Calls with run/2

The `Responses.run/2` function automates the process of handling function calls. It will repeatedly call your functions and feed the results back to the model until a final response is achieved:

```elixir
# Define available functions
functions = %{
  "get_weather" => fn %{"location" => location} ->
    # In a real app, this would call a weather API
    case location do
      "Paris" -> "15°C, partly cloudy"
      "London" -> "12°C, rainy"
      "New York" -> "8°C, sunny"
      _ -> "Weather data not available"
    end
  end,
  "get_time" => fn %{"timezone" => timezone} ->
    # In a real app, this would get actual time for timezone
    case timezone do
      "Europe/Paris" -> "14:30"
      "Europe/London" -> "13:30"
      "America/New_York" -> "08:30"
      _ -> "Unknown timezone"
    end
  end
}

# Define function tools
weather_tool = Responses.Schema.build_function(
  "get_weather",
  "Get current weather for a location",
  %{location: {:string, description: "City name"}}
)

time_tool = Responses.Schema.build_function(
  "get_time",
  "Get current time in a timezone",
  %{timezone: {:string, description: "Timezone like Europe/Paris"}}
)

# Run the conversation with automatic function calling
responses = Responses.run(
  [
    input: "What's the weather and time in Paris?",
    tools: [weather_tool, time_tool],
    model: "gpt-4.1-mini"
  ],
  functions
)

# The last response contains the final answer
responses |> List.last() |> Map.get(:text) |> IO.puts()

# You can also inspect all intermediate responses
IO.puts("\nTotal responses: #{length(responses)}")
```

The `run/2` function returns a list of all responses generated during the conversation. This allows you to:

* Track the conversation flow
* See what functions were called
* Calculate total costs across all API calls
* Debug issues in function calling

There's also a `run!/2` variant that raises on errors instead of returning error tuples.
